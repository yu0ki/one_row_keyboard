# -*- coding: utf-8 -*-
"""one_row_keyboard.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12SaWpnlnwAlVvRi9-0d6fX4weW25fhIO
"""
import sys
from preprocessors import *


# データセットの読み込み
from AsciiModDataset import AsciiModDataset
import pickle
import torch
import torch.nn as nn

# WikiTex2(DataPipe)をwordとascii mod encodeのリストに変換
# SENTENCE_LENGTH = 512
SENTENCE_LENGTH = 32

try: 
    with open('dataset.pickle', 'rb') as file:
        dataset = pickle.load(file)
except:
    dataset = AsciiModDataset(SENTENCE_LENGTH)
    # # ファイルにオブジェクトを保存する
    with open('dataset.pickle', 'wb') as file:
        pickle.dump(dataset, file)

# 学習データ、検証データに 8:2 の割合で分割する。
train_size = int(0.6 * len(dataset))
val_size = int(0.2 * len(dataset))
test_size = len(dataset) - train_size - val_size
train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(
    dataset, [train_size, val_size, test_size]
)

print(f"full: {len(dataset)} -> train: {len(train_dataset)}, valid: {len(val_dataset)}, test: {len(test_dataset)}")

# Dataloaderを使ってバッチ化
# https://zenn.dev/megane_otoko/articles/074_simple_pytorch
from torch.utils.data import DataLoader

# バッチサイズ
BATCH_SIZE = 128

# 学習用Dataloader
try:
    with open('train_dataloader.pickle', 'rb') as file:
        train_dataloader = pickle.load(file)
except:
    train_dataloader = DataLoader(
        train_dataset, 
        batch_size=BATCH_SIZE, 
        shuffle=True,
        num_workers=2, 
        drop_last=True,
        pin_memory=True
    )
    with open('train_dataloader.pickle', 'wb') as file:
        pickle.dump(train_dataloader, file)

# バリデーション用Dataloader
try:
    with open('val_dataloader.pickle', 'rb') as file:
        val_dataloader = pickle.load(file)
except:
    val_dataloader = DataLoader(
        val_dataset, 
        batch_size=BATCH_SIZE, 
        shuffle=True,
        num_workers=2, 
        drop_last=True,
        pin_memory=True
    )
    with open('val_dataloader.pickle', 'wb') as file:
        pickle.dump(val_dataloader, file)

# テスト用Dataloader
try:
    with open('test_dataloader.pickle', 'rb') as file:
        test_dataloader = pickle.load(file)
except:
    test_dataloader = DataLoader(
        test_dataset, 
        batch_size=BATCH_SIZE, 
        shuffle=True,
        num_workers=2, 
        drop_last=True,
        pin_memory=True
    )
    with open('test_dataloader.pickle', 'wb') as file:
        pickle.dump(test_dataloader, file)

# キャッシュクリア用
#  import torch._C as _C
# _C._jit_clear_class_registry()

# deviceを定義
device = torch.device("cuda:1" if torch.cuda.is_available() else "cpu")
print(device)

# モデルの定義
from KeyboardNet import KeyboardNet
num_class = 8 # 1文字のameに対応する文字の種類数
print("num_class: ", num_class)
mlp_hidden_layers = 120
model = KeyboardNet(SENTENCE_LENGTH, mlp_hidden_layers, num_class).float().to(device)



# 損失関数と最適化手法の設定
# https://torch.classcat.com/2021/04/18/pytorch-1-8-tutorials-beginner-transformer/

# https://tips-memo.com/python-point-loss-nan

criterion = nn.CrossEntropyLoss()


lr = 0.1 # learning rate
optimizer = torch.optim.SGD(model.parameters(), lr=lr)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.95)




from torchmetrics.classification import MulticlassAccuracy

from comet_ml import Experiment
from comet_ml.integration.pytorch import log_model

import json

# keys.json ファイルから APIキーとワークスペース名を読み込む
api_key = ""
workspace = ""
with open("keys.json", "r") as file:
    keys = json.load(file)
    api_key = keys["api_key"]
    workspace = keys["workspace"]

experiment = Experiment(
  api_key=api_key,
  project_name="one-row-keyboard",
  workspace=workspace
)

num_epochs = 50
best_accuracy = 0
metric = MulticlassAccuracy(num_classes=num_class).to(device)

hyper_params = {
    "sequence_length": SENTENCE_LENGTH,
    "input_size": SENTENCE_LENGTH,
    "bert_hidden_size": 768,
    "mlp_hidden_layers": mlp_hidden_layers,
    "num_classes": num_class,
    "batch_size": BATCH_SIZE,
    "num_epochs": num_epochs,
    "learning_rate": lr
}

experiment.log_parameters(hyper_params)
experiment.set_model_graph(str(model))

best_model_name = ""

with experiment.train():
    print("== Start Training ==")
    sys.stdout.flush()
    for epoch in range(num_epochs):
        print("Epoch: ", epoch + 1)
        sys.stdout.flush()
        batch_num = len(train_dataloader)

        model.train()  # モデルのトレーニングモードに切り替え

        for i, batch in enumerate(train_dataloader):
            # データの取得
            input_datas, target_datas = batch
            input_datas = input_datas.to(device)
            target_datas = target_datas.to(device)

            # 勾配の初期化
            optimizer.zero_grad()

            # モデルの順伝播
            attention_masks = []
            for target in target_datas:
                attention_masks.append([1 if t > 0 else 0 for t in target])

            outputs = model(input_datas, torch.tensor(attention_masks, dtype=torch.int64).to(device))

            # 損失の計算
            outputs_for_loss = outputs.view(-1, num_class).requires_grad_(True)
            target_for_loss = target_datas.view(-1)
            mask = torch.isin(target_for_loss, torch.tensor([1, 2, 3, 4, 5, 6, 7, 8]).to(device)).to(device)
            loss = criterion(outputs_for_loss[mask], target_for_loss[mask])

            # 勾配の計算とパラメータの更新
            loss.backward()
            optimizer.step()

            # 正解率の更新
            metric.update(outputs_for_loss[mask], target_for_loss[mask])

            # ログの出力
            if i % 500 == 1:
                print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}, Accuracy: {metric.compute()}")
                experiment.log_metric("loss", loss.item(), step=(epoch * len(train_dataloader) + i))
                sys.stdout.flush()
    

        # accuracyの計算
        acc = metric.compute().item()
        print("Accuracy:", acc)

        # 学習率の調整
        scheduler.step()

        # datasetの先頭のデータに対して復号
        for i in range(10, 14):
            chars, (input_ame, target_data) = dataset.get_char_and_dataset(i)
            print("input_sentence : ", chars)
            model_output = model(
                torch.tensor([input_ame.tolist()]).to(device), 
                torch.tensor([[1 if t > 0 else 0 for t in target_data]]).to(device)
            ).cpu()

            model_output = model_output.view(-1, num_class)
            result1 = []
            for a, o in zip(input_ame, model_output):
                if a not in [1, 2, 3, 4, 5, 6, 7, 8]:
                    result1.append(ame_to_possible_chars(a)[0])
                else:
                    ame = torch.argmax(o)
                    result1.append(ame_to_possible_chars(a)[ame])
            print("output_sentence: ", result1)

        # accuracyの計算
        train_acc = metric.compute().item()
        print("Training Accuracy:", train_acc)
        experiment.log_metric("traning_accuracy", train_acc, step=((epoch + 1) * len(train_dataloader)))

        # バリデーションセットでの正解率の計算
        model.eval()  # モデルを評価モードに切り替え
        val_metric = MulticlassAccuracy(num_classes=num_class).to(device) # バリデーションセット用のメトリックを初期化

        for val_batch in val_dataloader:
            val_input_datas, val_target_datas = val_batch
            val_input_datas = val_input_datas.to(device)
            val_target_datas = val_target_datas.to(device)

            val_attention_masks = []
            for target in val_target_datas:
                val_attention_masks.append([1 if t > 0 else 0 for t in target])

            val_outputs = model(val_input_datas, torch.tensor(val_attention_masks, dtype=torch.int64).to(device))

            val_outputs_for_loss = val_outputs.view(-1, num_class).requires_grad_(True)
            val_target_for_loss = val_target_datas.view(-1)
            val_mask = torch.isin(val_target_for_loss, torch.tensor([1, 2, 3, 4, 5, 6, 7, 8]).to(device)).to(device)
            val_metric.update(val_outputs_for_loss[val_mask], val_target_for_loss[val_mask])

        val_acc = val_metric.compute().item()

        # val_accucacyが最大の場合は保存
        if best_accuracy < val_acc:
            best_accuracy = val_acc
            best_model = model.state_dict()

            # 最も良い精度を持つモデルをファイルに保存する
            best_model_name = 'best_model_accuracy_' + str(best_accuracy)
            if val_acc > 0.68:
                print("BEST MODEL UPDATED!")
                torch.save(best_model, best_model_name)
            else:
                print("best val accuracy updated")

        print("Validation Accuracy:", val_acc)
        experiment.log_metric("val_accuracy", val_acc, step=((epoch + 1) * len(train_dataloader)))

        metric.reset()
        val_metric.reset()
        

# test
print("=== Test ===")
model.eval()
test_metric = MulticlassAccuracy(num_classes=num_class).to(device) # テストセット用のメトリックを初期化
with experiment.test():
    best_test_model = KeyboardNet(SENTENCE_LENGTH=SENTENCE_LENGTH, mlp_hidden_layers=mlp_hidden_layers, num_class=8)
    best_test_model.load_state_dict(torch.load(best_model_name))
    log_model(experiment, best_test_model, model_name=best_model_name)
    best_test_model = best_test_model.to(device)

    for test_batch in test_dataloader:
        test_input_datas, test_target_datas = test_batch
        test_input_datas = test_input_datas.to(device)
        test_target_datas = test_target_datas.to(device)

        test_attention_masks = []
        for target in test_target_datas:
            test_attention_masks.append([1 if t > 0 else 0 for t in target])

        test_outputs = best_test_model(test_input_datas, torch.tensor(test_attention_masks, dtype=torch.int64).to(device))

        test_outputs_for_loss = test_outputs.view(-1, num_class).requires_grad_(True)
        test_target_for_loss = test_target_datas.view(-1)
        test_mask = torch.isin(test_target_for_loss, torch.tensor([1, 2, 3, 4, 5, 6, 7, 8]).to(device)).to(device)
        test_metric.update(test_outputs_for_loss[test_mask], test_target_for_loss[test_mask])

    experiment.log_metric("test_accuracy_of_best_model", test_metric.compute().item())
    print("best model name: ", best_model_name)
    print("test accuracy of best model: ", test_metric.compute().item())
